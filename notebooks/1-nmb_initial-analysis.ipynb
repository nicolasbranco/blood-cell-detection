{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial analysis\n",
    "\n",
    "## Executive summary\n",
    "\n",
    "- most important bullet points\n",
    "- the validation dataset will be ignored\n",
    "\n",
    "## More information\n",
    "\n",
    "**1 - why ignore the validation dataset**\n",
    "\n",
    "As observed and explained on the dataset & paper, the validation dataset was a part of the training set. However this makes no sense, so we'll re-split the train set into a new train & validation, find the best hyperparameters based on that, then finally retrain with the best parameters on the train+validation sets.\n",
    "\n",
    "**2 - sencond point**\n",
    "\n",
    "xxxx\n",
    "\n",
    "- xxxx\n",
    "- xxxx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports & configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#### default imports ####\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### specific imports ###\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# forces local code to be reloaded to avoid problems\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#### important configs ####\n",
    "# uses seaborn configs for prettier graphs\n",
    "sns.set_theme()\n",
    "# shows thousand separator for values\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "# enable import from src/\n",
    "sys.path.append('..')  \n",
    "\n",
    "#### paths ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## auxiliar functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - validation dataset problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "xml                  420\n",
       "jpg                  420\n",
       "gitkeep                1\n",
       "/data/raw/LICENSE      1\n",
       "md                     1\n",
       "yml                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where the images & labels are\n",
    "raw_path = \"../data/raw\"\n",
    "\n",
    "# gets the path for all files\n",
    "df_all = pd.DataFrame()\n",
    "for dirname, _, filenames in os.walk(raw_path):\n",
    "    paths = [dirname + \"/\" + filename for filename in filenames]\n",
    "    folder_name = os.path.split(dirname)[-1]\n",
    "    df_all = pd.concat([df_all, pd.DataFrame({\"path\": paths})], ignore_index=True)\n",
    "\n",
    "# transforms to df\n",
    "df_all = pd.DataFrame(df_all)\n",
    "\n",
    "# also gets the filename\n",
    "df_all[\"filename\"] = df_all[\"path\"].apply(lambda s: s.split(\"/\")[-1])\n",
    "\n",
    "# and finally check possible extensions\n",
    "extensions = df_all[\"path\"].apply(lambda s: s.split(\".\")[-1])\n",
    "extensions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "Training      600\n",
       "Testing       120\n",
       "Validation    120\n",
       "data            4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a reference for the dataset (which folder it is from)\n",
    "df_all[\"dataset\"] = df_all[\"path\"].apply(lambda s: s.split(\"/\")[-3])\n",
    "df_all[\"dataset\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all the files in validation dataset are also in the training one\n",
    "for filename in df_all[df_all[\"dataset\"] == \"Validation\"][\"filename\"]:\n",
    "    if filename not in df_all[df_all[\"dataset\"] == \"Training\"][\"filename\"].values:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Here it is possible to observe that all files in the validation folder are (as explained in the paper & GitHub) duplicated from the training dataset. This utilization is a methodological problem, so will not be used in our study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Basic information about the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"../data/raw/Training/Images/BloodImage_00000.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/Training/Images/BloodImage_00000.jpg\n"
     ]
    }
   ],
   "source": [
    "!ls \"../data/raw/Training/Images/BloodImage_00000.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with os.scandir(\"../data/raw/Training/Images/\") as entries:\n",
    "#     for entry in entries:\n",
    "#         print(entry.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bc_detec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
